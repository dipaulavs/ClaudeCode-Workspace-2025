#!/usr/bin/env python3
"""
Instagram Scraper via Apify API

Extrai dados p√∫blicos do Instagram: posts, coment√°rios, perfis, hashtags e localiza√ß√µes.

Uso:
    # Scrape posts de usu√°rio
    python3 apify_instagram.py --user "natgeo" --results-type posts --limit 50

    # Scrape coment√°rios de post
    python3 apify_instagram.py --url "https://instagram.com/p/ABC123/" --results-type comments

    # Scrape posts de hashtag
    python3 apify_instagram.py --hashtag "endgame" --results-type posts --limit 100

    # Scrape detalhes de perfil
    python3 apify_instagram.py --user "avengers" --results-type details

    # Scrape posts de localiza√ß√£o
    python3 apify_instagram.py --place "Niagara Falls" --results-type posts --limit 50

Autor: Claude Code
Data: 2025-11-02
"""

import sys
import os
import json
import time
import argparse
from datetime import datetime

# Adicionar diret√≥rio raiz ao path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Importar configura√ß√µes
from config.apify_config import (
    APIFY_API_KEY,
    INSTAGRAM_SCRAPER_ACTOR_ID,
    INSTAGRAM_DEFAULTS,
    INSTAGRAM_SEARCH_TYPES,
    INSTAGRAM_RESULTS_TYPES,
    DEFAULT_TIMEOUT,
    EXPORT_DIR
)

try:
    from apify_client import ApifyClient
except ImportError:
    print("‚ùå Erro: Biblioteca 'apify-client' n√£o instalada")
    print("üì¶ Instale com: pip3 install apify-client")
    sys.exit(1)


class InstagramScraper:
    """Cliente para Instagram Scraper da Apify"""

    def __init__(self, api_key=None):
        """
        Inicializa o cliente

        Args:
            api_key: API key da Apify (opcional, usa vari√°vel de ambiente se n√£o fornecido)
        """
        self.api_key = api_key or APIFY_API_KEY
        if not self.api_key:
            raise ValueError("‚ùå APIFY_API_KEY n√£o configurada")

        self.client = ApifyClient(self.api_key)
        self.actor = self.client.actor(INSTAGRAM_SCRAPER_ACTOR_ID)

    def scrape(
        self,
        user=None,
        hashtag=None,
        place=None,
        url=None,
        results_type="posts",
        results_limit=50,
        search_limit=10,
        newer_than=None,
        older_than=None,
        timeout=None,
        output_file=None
    ):
        """
        Executa scraping do Instagram

        Args:
            user: Username do Instagram (sem @)
            hashtag: Hashtag (sem #)
            place: Nome da localiza√ß√£o
            url: URL direta de post (para scrape de coment√°rios)
            results_type: Tipo de resultado ("posts", "comments", "details")
            results_limit: Limite de resultados (posts/coment√°rios)
            search_limit: Limite de resultados de busca (hashtags/places)
            newer_than: Data ISO 8601 - apenas posts mais novos que (ex: "2024-01-01")
            older_than: Data ISO 8601 - apenas posts mais antigos que
            timeout: Timeout em segundos
            output_file: Caminho para salvar resultado (JSON)

        Returns:
            dict: Resultado do scraping
        """
        # Valida√ß√µes
        if not any([user, hashtag, place, url]):
            raise ValueError("‚ùå Forne√ßa pelo menos um: user, hashtag, place ou url")

        if results_type not in INSTAGRAM_RESULTS_TYPES.values():
            raise ValueError(f"‚ùå results_type inv√°lido. Use: {list(INSTAGRAM_RESULTS_TYPES.values())}")

        # Montar input
        run_input = {
            "resultsType": results_type,
            "resultsLimit": results_limit,
            "searchLimit": search_limit,
            **INSTAGRAM_DEFAULTS
        }

        # Adicionar filtros de data
        if newer_than:
            run_input["onlyPostsNewerThan"] = newer_than
        if older_than:
            run_input["onlyPostsOlderThan"] = older_than

        # Adicionar tipo de busca
        if url:
            # URL direta (geralmente para coment√°rios)
            run_input["directUrls"] = [url] if isinstance(url, str) else url
        elif user:
            run_input["search"] = user
            run_input["searchType"] = INSTAGRAM_SEARCH_TYPES["user"]
        elif hashtag:
            # Remover # se fornecido
            hashtag = hashtag.lstrip('#')
            run_input["search"] = hashtag
            run_input["searchType"] = INSTAGRAM_SEARCH_TYPES["hashtag"]
        elif place:
            run_input["search"] = place
            run_input["searchType"] = INSTAGRAM_SEARCH_TYPES["place"]

        # Executar
        print(f"üöÄ Iniciando scraping do Instagram...")
        print(f"üìã Configura√ß√£o:")
        if user:
            print(f"   üë§ Usu√°rio: @{user}")
        if hashtag:
            print(f"   #Ô∏è‚É£ Hashtag: #{hashtag}")
        if place:
            print(f"   üìç Localiza√ß√£o: {place}")
        if url:
            print(f"   üîó URL: {url}")
        print(f"   üìä Tipo: {results_type}")
        print(f"   üî¢ Limite: {results_limit}")

        try:
            # Executar actor
            run = self.actor.call(
                run_input=run_input,
                timeout_secs=timeout or DEFAULT_TIMEOUT
            )

            # Buscar resultados
            print(f"\n‚è≥ Aguardando resultados...")
            dataset = self.client.dataset(run["defaultDatasetId"])
            items = list(dataset.iterate_items())

            result = {
                "success": True,
                "run_id": run["id"],
                "status": run["status"],
                "items_count": len(items),
                "items": items,
                "stats": {
                    "started_at": run.get("startedAt"),
                    "finished_at": run.get("finishedAt"),
                    "compute_units": run.get("stats", {}).get("computeUnits", 0)
                }
            }

            # Salvar em arquivo se solicitado
            if output_file:
                output_path = os.path.expanduser(output_file)
                os.makedirs(os.path.dirname(output_path), exist_ok=True)

                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False)

                print(f"\n‚úÖ Resultados salvos em: {output_path}")

            # Resumo
            print(f"\n‚úÖ Scraping conclu√≠do!")
            print(f"üìä Total de itens: {len(items)}")
            print(f"‚ö° Compute units: {result['stats']['compute_units']:.4f}")

            return result

        except Exception as e:
            print(f"\n‚ùå Erro no scraping: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "items": []
            }

    def scrape_user_posts(self, username, limit=50, output_file=None):
        """
        Atalho: Scrape posts de usu√°rio

        Args:
            username: Username (sem @)
            limit: Limite de posts
            output_file: Caminho para salvar resultado

        Returns:
            dict: Resultado do scraping
        """
        return self.scrape(
            user=username,
            results_type="posts",
            results_limit=limit,
            output_file=output_file
        )

    def scrape_hashtag_posts(self, hashtag, limit=50, output_file=None):
        """
        Atalho: Scrape posts de hashtag

        Args:
            hashtag: Hashtag (sem #)
            limit: Limite de posts
            output_file: Caminho para salvar resultado

        Returns:
            dict: Resultado do scraping
        """
        return self.scrape(
            hashtag=hashtag,
            results_type="posts",
            results_limit=limit,
            output_file=output_file
        )

    def scrape_post_comments(self, post_url, limit=50, output_file=None):
        """
        Atalho: Scrape coment√°rios de post

        Args:
            post_url: URL do post
            limit: Limite de coment√°rios
            output_file: Caminho para salvar resultado

        Returns:
            dict: Resultado do scraping
        """
        return self.scrape(
            url=post_url,
            results_type="comments",
            results_limit=limit,
            output_file=output_file
        )

    def scrape_user_profile(self, username, output_file=None):
        """
        Atalho: Scrape detalhes de perfil

        Args:
            username: Username (sem @)
            output_file: Caminho para salvar resultado

        Returns:
            dict: Resultado do scraping
        """
        return self.scrape(
            user=username,
            results_type="details",
            output_file=output_file
        )


def main():
    """Fun√ß√£o principal para CLI"""
    parser = argparse.ArgumentParser(
        description="Instagram Scraper via Apify API",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:

  # Scrape posts de usu√°rio
  python3 apify_instagram.py --user "natgeo" --results-type posts --limit 50

  # Scrape coment√°rios de post
  python3 apify_instagram.py --url "https://instagram.com/p/ABC123/" --results-type comments

  # Scrape posts de hashtag
  python3 apify_instagram.py --hashtag "travel" --results-type posts --limit 100

  # Scrape detalhes de perfil
  python3 apify_instagram.py --user "avengers" --results-type details

  # Scrape posts de localiza√ß√£o
  python3 apify_instagram.py --place "Niagara Falls" --results-type posts --limit 50

  # Filtrar por data (apenas posts ap√≥s 2024-01-01)
  python3 apify_instagram.py --user "natgeo" --newer-than "2024-01-01" --limit 50

Tipos de resultado:
  - posts: Retorna posts (imagens/v√≠deos/carrosseis)
  - comments: Retorna coment√°rios (requer URL de post)
  - details: Retorna detalhes (perfil/hashtag/localiza√ß√£o)
        """
    )

    # Argumentos principais
    parser.add_argument('--user', help='Username do Instagram (sem @)')
    parser.add_argument('--hashtag', help='Hashtag (sem #)')
    parser.add_argument('--place', help='Nome da localiza√ß√£o')
    parser.add_argument('--url', help='URL direta de post (para coment√°rios)')

    # Configura√ß√µes
    parser.add_argument(
        '--results-type',
        choices=['posts', 'comments', 'details'],
        default='posts',
        help='Tipo de resultado (padr√£o: posts)'
    )
    parser.add_argument('--limit', type=int, default=50, help='Limite de resultados (padr√£o: 50)')
    parser.add_argument('--search-limit', type=int, default=10, help='Limite de busca (padr√£o: 10)')

    # Filtros de data
    parser.add_argument('--newer-than', help='Apenas posts mais novos que (ISO 8601: 2024-01-01)')
    parser.add_argument('--older-than', help='Apenas posts mais antigos que (ISO 8601: 2024-01-01)')

    # Output
    parser.add_argument(
        '--output',
        help='Arquivo de sa√≠da (padr√£o: ~/Downloads/instagram_TIMESTAMP.json)'
    )
    parser.add_argument('--timeout', type=int, help='Timeout em segundos')

    # Outros
    parser.add_argument('--api-key', help='API Key da Apify (opcional)')

    args = parser.parse_args()

    # Valida√ß√µes
    if not any([args.user, args.hashtag, args.place, args.url]):
        parser.error("‚ùå Forne√ßa pelo menos um: --user, --hashtag, --place ou --url")

    # Gerar nome de arquivo se n√£o fornecido
    if not args.output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        source = args.user or args.hashtag or args.place or "post"
        filename = f"instagram_{source}_{args.results_type}_{timestamp}.json"
        args.output = os.path.join(EXPORT_DIR, filename)

    # Executar scraping
    scraper = InstagramScraper(api_key=args.api_key)

    result = scraper.scrape(
        user=args.user,
        hashtag=args.hashtag,
        place=args.place,
        url=args.url,
        results_type=args.results_type,
        results_limit=args.limit,
        search_limit=args.search_limit,
        newer_than=args.newer_than,
        older_than=args.older_than,
        timeout=args.timeout,
        output_file=args.output
    )

    # Exit code
    sys.exit(0 if result.get("success") else 1)


if __name__ == "__main__":
    main()
