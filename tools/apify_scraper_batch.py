#!/usr/bin/env python3
"""
Apify Web Scraper - Batch (M√∫ltiplas URLs)

Permite fazer scraping de m√∫ltiplas URLs em sequ√™ncia, salvando cada uma
em sua pr√≥pria pasta organizada.

Uso:
    python3 tools/apify_scraper_batch.py "URL1" "URL2" "URL3" ...
"""

import sys
import os
from datetime import datetime

# Adiciona o diret√≥rio pai ao path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from apify_scraper import ApifyScraper


def main():
    """Fun√ß√£o principal para scraping em batch"""
    if len(sys.argv) < 2:
        print("Uso: python3 tools/apify_scraper_batch.py 'URL1' 'URL2' 'URL3' ...")
        print("\nExemplos:")
        print("  python3 tools/apify_scraper_batch.py 'https://docs.site1.com' 'https://docs.site2.com'")
        sys.exit(1)

    urls = sys.argv[1:]

    # Valida URLs
    for url in urls:
        if not url.startswith(('http://', 'https://')):
            print(f"‚ùå Erro: URL inv√°lida '{url}' - deve come√ßar com http:// ou https://")
            sys.exit(1)

    print("=" * 80)
    print("üåê APIFY WEB SCRAPER - BATCH MODE")
    print("=" * 80)
    print(f"\nüìã Total de URLs para processar: {len(urls)}\n")

    for idx, url in enumerate(urls, 1):
        print(f"   {idx}. {url}")

    print(f"\n{'=' * 80}")

    # Inicializa scraper
    scraper = ApifyScraper()

    # Processa cada URL
    results_summary = []
    start_time = datetime.now()

    for idx, url in enumerate(urls, 1):
        print(f"\n\n{'#' * 80}")
        print(f"# URL {idx}/{len(urls)}")
        print(f"{'#' * 80}\n")

        try:
            # Preview
            preview = scraper.preview_scrape(url, max_preview_pages=50)

            if not preview["success"]:
                print(f"‚ùå Erro no preview: {preview['error']}")
                results_summary.append({
                    "url": url,
                    "success": False,
                    "error": preview['error']
                })
                continue

            num_pages = preview["num_pages"]
            print(f"üìä P√°ginas encontradas: {num_pages}")

            # Scraping completo
            print(f"\nüöÄ Iniciando scraping...")
            results = scraper.scrape(url)

            if results["success"]:
                # Salva resultados
                output_path = scraper.save_results(url, results)
                results_summary.append({
                    "url": url,
                    "success": True,
                    "pages": results["num_pages"],
                    "output_path": output_path
                })
                print(f"‚úÖ Conclu√≠do! ({results['num_pages']} p√°ginas)")
            else:
                print(f"‚ùå Erro no scraping: {results['error']}")
                results_summary.append({
                    "url": url,
                    "success": False,
                    "error": results['error']
                })

        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Interrompido pelo usu√°rio. Salvando progresso at√© aqui...")
            break
        except Exception as e:
            print(f"‚ùå Erro inesperado: {str(e)}")
            results_summary.append({
                "url": url,
                "success": False,
                "error": str(e)
            })

    # Resumo final
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print(f"\n\n{'=' * 80}")
    print("üìä RESUMO FINAL")
    print("=" * 80)
    print(f"\n‚è±Ô∏è  Tempo total: {duration:.1f} segundos")
    print(f"üìã URLs processadas: {len(results_summary)}/{len(urls)}\n")

    successful = [r for r in results_summary if r["success"]]
    failed = [r for r in results_summary if not r["success"]]

    print(f"‚úÖ Sucessos: {len(successful)}")
    for result in successful:
        print(f"   ‚Ä¢ {result['url']} ({result['pages']} p√°ginas)")
        print(f"     ‚îî‚îÄ {result['output_path']}")

    if failed:
        print(f"\n‚ùå Falhas: {len(failed)}")
        for result in failed:
            print(f"   ‚Ä¢ {result['url']}")
            print(f"     ‚îî‚îÄ Erro: {result['error']}")

    print("\n" + "=" * 80)

    if failed:
        print(f"‚ö†Ô∏è  Aten√ß√£o: {len(failed)} URL(s) falharam")
    else:
        print("‚úÖ Todas as URLs foram processadas com sucesso!")

    print("=" * 80)


if __name__ == "__main__":
    main()
