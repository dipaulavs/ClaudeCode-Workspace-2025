#!/usr/bin/env python3
"""
Template: Web Scraping de Sites
Extrai conte√∫do de websites completos e salva em Markdown via Apify

Uso:
    python3 scripts/extraction/scrape_website.py "URL_DO_SITE"
    python3 scripts/extraction/scrape_website.py "https://docs.site.com" --max-pages 100
"""

import sys
import os
from pathlib import Path

# Adiciona tools/ ao path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "tools"))

try:
    from apify_scraper import ApifyScraper
except ImportError:
    print("‚ùå Erro: N√£o foi poss√≠vel importar apify_scraper.py")
    print("Verifique se o arquivo existe em: tools/apify_scraper.py")
    sys.exit(1)

import argparse


def main():
    parser = argparse.ArgumentParser(
        description="Faz scraping de websites completos e salva em Markdown",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos:
  # Scraping b√°sico (ilimitado)
  python3 scripts/extraction/scrape_website.py "https://docs.example.com"

  # Limitar quantidade de p√°ginas
  python3 scripts/extraction/scrape_website.py "https://docs.site.com" --max-pages 50

  # Controlar profundidade de crawling
  python3 scripts/extraction/scrape_website.py "https://site.com" --max-depth 3

  # Combinar limites
  python3 scripts/extraction/scrape_website.py "https://site.com" --max-pages 100 --max-depth 5

Recursos:
  ‚Ä¢ Preview autom√°tico antes de executar
  ‚Ä¢ Segue links internos automaticamente
  ‚Ä¢ Converte tudo para Markdown
  ‚Ä¢ Salva p√°ginas individuais + conte√∫do completo
  ‚Ä¢ Inclui metadata (t√≠tulos, URLs, timestamps)
  ‚Ä¢ Arquivos salvos em: ~/Downloads/apify_scrape_DOMAIN_TIMESTAMP/

Notas:
  ‚Ä¢ Ideal para documenta√ß√µes t√©cnicas
  ‚Ä¢ Respeita estrutura de links do site
  ‚Ä¢ Ignora recursos externos (imagens, CSS, JS)
        """
    )

    parser.add_argument(
        "url",
        help="URL do site para fazer scraping"
    )

    parser.add_argument(
        "--max-pages",
        type=int,
        default=None,
        help="Limite m√°ximo de p√°ginas (padr√£o: ilimitado)"
    )

    parser.add_argument(
        "--max-depth",
        type=int,
        default=None,
        help="Profundidade m√°xima de crawl (padr√£o: config padr√£o)"
    )

    parser.add_argument(
        "--no-preview",
        action="store_true",
        help="Pular preview e executar direto"
    )

    args = parser.parse_args()

    # Valida URL
    if not args.url.startswith(('http://', 'https://')):
        print("‚ùå Erro: URL deve come√ßar com http:// ou https://")
        sys.exit(1)

    print("=" * 80)
    print("üåê WEB SCRAPING")
    print("=" * 80)

    # Inicializa scraper
    scraper = ApifyScraper()

    # Preview (se n√£o pulado)
    if not args.no_preview:
        preview = scraper.preview_scrape(args.url, max_preview_pages=50)

        if not preview["success"]:
            print(f"‚ùå Erro no preview: {preview['error']}")
            sys.exit(1)

        num_pages = preview["num_pages"]
        print(f"üìä P√°ginas encontradas (preview): {num_pages}")

        if preview["reached_limit"]:
            print(f"‚ö†Ô∏è  O site pode ter MAIS de {num_pages} p√°ginas")

        # Mostra amostra
        print(f"\nüìù Exemplos de p√°ginas:")
        for idx, url in enumerate(preview["sample_urls"][:5], 1):
            print(f"   {idx}. {url}")

        # Confirma√ß√£o
        print(f"\n{'=' * 80}")
        try:
            response = input("Continuar com scraping completo? (s/n): ").strip().lower()
        except KeyboardInterrupt:
            print("\n\n‚ùå Cancelado pelo usu√°rio.")
            sys.exit(0)

        if response != 's':
            print("‚ùå Opera√ß√£o cancelada.")
            sys.exit(0)

        print(f"\n{'=' * 80}")

    # Scraping completo
    results = scraper.scrape(args.url, max_pages=args.max_pages, max_depth=args.max_depth)

    if results["success"]:
        # Salva resultados
        output_path = scraper.save_results(args.url, results)
        print("=" * 80)
        print("‚úÖ Scraping conclu√≠do com sucesso!")
        print("=" * 80)
    else:
        print(f"‚ùå Erro no scraping: {results['error']}")
        sys.exit(1)


if __name__ == "__main__":
    main()
