#!/usr/bin/env python3
"""
Template: Web Scraping em Batch (M√∫ltiplas URLs)
Extrai conte√∫do de m√∫ltiplos sites em sequ√™ncia via Apify

Uso:
    python3 scripts/extraction/scrape_batch.py "URL1" "URL2" "URL3"
"""

import sys
import os
from pathlib import Path
from datetime import datetime

# Adiciona tools/ ao path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "tools"))

try:
    from apify_scraper import ApifyScraper
except ImportError:
    print("‚ùå Erro: N√£o foi poss√≠vel importar apify_scraper.py")
    print("Verifique se o arquivo existe em: tools/apify_scraper.py")
    sys.exit(1)


def main():
    if len(sys.argv) < 2:
        print("Uso: python3 scripts/extraction/scrape_batch.py 'URL1' 'URL2' 'URL3' ...")
        print("\nExemplos:")
        print("  # Scraping de 2 sites")
        print("  python3 scripts/extraction/scrape_batch.py 'https://docs.site1.com' 'https://docs.site2.com'")
        print("\n  # Scraping de m√∫ltiplas documenta√ß√µes")
        print("  python3 scripts/extraction/scrape_batch.py \\")
        print("    'https://docs.react.dev' \\")
        print("    'https://docs.python.org/3' \\")
        print("    'https://nodejs.org/docs'")
        print("\nRecursos:")
        print("  ‚Ä¢ Processa cada URL em sequ√™ncia")
        print("  ‚Ä¢ Salva cada site em pasta separada")
        print("  ‚Ä¢ Preview autom√°tico para cada site")
        print("  ‚Ä¢ Resumo final com estat√≠sticas")
        print("  ‚Ä¢ Tratamento de erros individual")
        sys.exit(1)

    urls = sys.argv[1:]

    # Valida URLs
    for url in urls:
        if not url.startswith(('http://', 'https://')):
            print(f"‚ùå Erro: URL inv√°lida '{url}'")
            print("   URLs devem come√ßar com http:// ou https://")
            sys.exit(1)

    print("=" * 80)
    print("üåê WEB SCRAPING - MODO BATCH")
    print("=" * 80)
    print(f"\nüìã Total de URLs: {len(urls)}\n")

    for idx, url in enumerate(urls, 1):
        print(f"   {idx}. {url}")

    print(f"\n{'=' * 80}")

    # Inicializa scraper
    scraper = ApifyScraper()

    # Processa cada URL
    results_summary = []
    start_time = datetime.now()

    for idx, url in enumerate(urls, 1):
        print(f"\n\n{'#' * 80}")
        print(f"# URL {idx}/{len(urls)}")
        print(f"{'#' * 80}\n")

        try:
            # Preview
            preview = scraper.preview_scrape(url, max_preview_pages=50)

            if not preview["success"]:
                print(f"‚ùå Erro no preview: {preview['error']}")
                results_summary.append({
                    "url": url,
                    "success": False,
                    "error": preview['error']
                })
                continue

            num_pages = preview["num_pages"]
            print(f"üìä P√°ginas encontradas: {num_pages}")

            # Scraping completo (sem confirma√ß√£o no batch)
            print(f"\nüöÄ Iniciando scraping autom√°tico...")
            results = scraper.scrape(url)

            if results["success"]:
                # Salva resultados
                output_path = scraper.save_results(url, results)
                results_summary.append({
                    "url": url,
                    "success": True,
                    "pages": results["num_pages"],
                    "output_path": output_path
                })
                print(f"‚úÖ Conclu√≠do! ({results['num_pages']} p√°ginas)")
            else:
                print(f"‚ùå Erro no scraping: {results['error']}")
                results_summary.append({
                    "url": url,
                    "success": False,
                    "error": results['error']
                })

        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Interrompido pelo usu√°rio.")
            print("   Salvando progresso...")
            break
        except Exception as e:
            print(f"‚ùå Erro inesperado: {str(e)}")
            results_summary.append({
                "url": url,
                "success": False,
                "error": str(e)
            })

    # Resumo final
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print(f"\n\n{'=' * 80}")
    print("üìä RESUMO FINAL")
    print("=" * 80)
    print(f"\n‚è±Ô∏è  Tempo total: {duration:.1f} segundos")
    print(f"üìã URLs processadas: {len(results_summary)}/{len(urls)}\n")

    successful = [r for r in results_summary if r["success"]]
    failed = [r for r in results_summary if not r["success"]]

    print(f"‚úÖ Sucessos: {len(successful)}")
    for result in successful:
        print(f"   ‚Ä¢ {result['url']} ({result['pages']} p√°ginas)")
        print(f"     ‚îî‚îÄ {result['output_path']}")

    if failed:
        print(f"\n‚ùå Falhas: {len(failed)}")
        for result in failed:
            print(f"   ‚Ä¢ {result['url']}")
            print(f"     ‚îî‚îÄ Erro: {result['error']}")

    print("\n" + "=" * 80)

    if failed:
        print(f"‚ö†Ô∏è  {len(failed)} URL(s) falharam")
    else:
        print("‚úÖ Todas as URLs processadas com sucesso!")

    print("=" * 80)


if __name__ == "__main__":
    main()
